{
  "level": 3,
  "title": "AI, Child Safety & Burden of Responsibility",
  "scenarios": [
    {
      "scenario_id": "L3-S1",
      "title": "Hash or Hesitate?",
      "prompt": "A social media platform must choose between a fast AI method on users' devices to quickly detect child sexual abuse material (CSAM), which raises privacy concerns, and a slower server method that protects privacy but delays removal of harmful content. What would be your approach?",
      "choices": {
        "A": "Perform scans directly on user devices using checks from reliable sources, ensuring all requests are encrypted to maintain privacy and security.",
        "B": "Implement server-side scanning with takedown measures, create educational resources, and share information on harmful content.",
        "C": "Customize scanning protocols based on environments: some allow user device scans, while others require server scans."
      },
      "toolkit_cues": "How do we balance child safety with privacy by default? What mix limits harm?",
      "p3_cues": "People (victim safety), Planet (scale inference), Parity (over-policing certain groups).",
      "toolkit_flow": {
        "order": [],
        "prompts": [],
        "quick_actions": {
          "A": [
            "Ship encrypted on-device checks",
            "Provide clear false-positive appeals",
            "Monitor device performance impact"
          ],
          "B": [
            "Centralize scanning quality controls",
            "Launch survivor resource hub",
            "Audit access + retention paths"
          ],
          "C": [
            "Define when device vs. server applies",
            "Publish user-readable policy visuals",
            "Run periodic gap analysis"
          ]
        },
        "metrics": [
          "Detection accuracy rate",
          "False positive rate",
          "Appeal success rate"
        ],
        "owner_required": true,
        "review_default_days": 90
      },
      "toolkit_references": "Values Explainer Cards, Weighing Options"
    },
    {
      "scenario_id": "L3-S2",
      "title": "Safer Generative AI (Gen-AI)",
      "prompt": "Recent findings indicate that a creative generative AI tool can be manipulated into producing inappropriate content involving minors when subjected to adversarial prompts. What course of action would you recommend?",
      "choices": {
        "A": "Implement safety protocols, use watermarking for transparency, and create fast appeal processes for flagged content.",
        "B": "Refine datasets to exclude harmful material, slow release pace, and conduct red-teaming before public deployment.",
        "C": "Restrict access to potentially sensitive features, allowing only verified researchers to utilize them for responsible experimentation."
      },
      "toolkit_cues": "What abuse paths and secondary-trauma risks exist? What gates and response plans are required?",
      "p3_cues": "People (reviewer well-being), Planet (retraining cycles), Parity (access for small orgs).",
      "toolkit_flow": {
        "order": [],
        "prompts": [],
        "quick_actions": {
          "A": [
            "Enforce safety policy + rate limits",
            "Embed robust watermarking",
            "Staff rapid appeals workflow"
          ],
          "B": [
            "Clean datasets against risky content",
            "Conduct external red-team exercises",
            "Gate new features behind safety checks"
          ],
          "C": [
            "Vet researcher access criteria",
            "Log + review sensitive usage",
            "Publish periodic safety findings"
          ]
        },
        "metrics": [
          "Safety protocol compliance %",
          "False positive rate",
          "Red-team findings"
        ],
        "owner_required": true,
        "review_default_days": 60
      },
      "toolkit_references": "Future Story, Ethics Frame"
    },
    {
      "scenario_id": "L3-S3",
      "title": "Reports at Scale",
      "prompt": "Law enforcement has requested the ability to submit bulk uploads to the platform's abuse Application Programming Interface (API). However, this may lead to an increase in false reports. What would be your recommended approach?",
      "choices": {
        "A": "Implement bulk uploads but require human review and establish rate limits to prevent abuse.",
        "B": "Prioritize matches that have high confidence levels and establish strong processes for handling appeals.",
        "C": "Direct submissions through an independent clearinghouse to verify reports before they are processed."
      },
      "toolkit_cues": "Do faster reports outweigh wrongful flags? Who reviews, how fast, and with what limits?",
      "p3_cues": "People (mislabeling harm), Planet (processing load), Parity (small platform capacity).",
      "toolkit_flow": {
        "order": [],
        "prompts": [],
        "quick_actions": {
          "A": [
            "Implement rate-limited intake API",
            "Require human triage on batches",
            "Track reviewer load + trauma supports"
          ],
          "B": [
            "Set confidence threshold for action",
            "Provide creator/user appeal SLAs",
            "Audit false-positive/negative rates"
          ],
          "C": [
            "Route reports via third-party vetting",
            "Define shared evidence standards",
            "Publish throughput + accuracy stats"
          ]
        },
        "metrics": [
          "False report rate",
          "Appeal success rate",
          "Processing time"
        ],
        "owner_required": true,
        "review_default_days": 90
      },
      "toolkit_references": "Impact Explorer, Ethics Frame"
    },
    {
      "scenario_id": "L3-S4",
      "title": "Schools use AI",
      "prompt": "Several middle schools are using AI to monitor students to prevent violence. A recent investigation has uncovered potential security risks associated with this practice.",
      "choices": {
        "A": "Advocate for stronger regulations on AI use in schools to prioritize student privacy and security.",
        "B": "Encourage schools to be transparent about AI monitoring practices and engage in open conversations with parents and students.",
        "C": "Propose alternative measures for ensuring student safety that do not involve AI monitoring, like enhanced conflict resolution programs and better mental health support."
      },
      "toolkit_cues": "Is monitoring the least intrusive means for safety? What non-AI measures could replace it?",
      "p3_cues": "People (student trust), Planet (data storage), Parity (guardian consent gaps).",
      "toolkit_flow": {
        "order": [],
        "prompts": [],
        "quick_actions": {
          "A": [
            "Adopt district-wide guardrails",
            "Post parent/student rights notice",
            "Schedule annual compliance audits"
          ],
          "B": [
            "Hold parent/student info sessions",
            "Offer opt-outs + alternatives",
            "Publish data use + retention policies"
          ],
          "C": [
            "Expand counseling/mediation staff",
            "Implement threat reporting training",
            "Track incidents + response timelines"
          ]
        },
        "metrics": [
          "Security incident rate",
          "Consent rate %",
          "Alternative program effectiveness"
        ],
        "owner_required": true,
        "review_default_days": 60
      },
      "toolkit_references": "Values Explainer Cards, Weighing Options"
    },
    {
      "scenario_id": "L3-S5",
      "title": "Survivor-First Takedown",
      "prompt": "Victims of online harassment seek quick removal of harmful content, while some agencies may need copies for investigations, balancing swift action with the need to preserve evidence. What would your decision be?",
      "choices": {
        "A": "Implement a rapid removal process, retaining only hashed proof for verification.",
        "B": "Conduct a gradual removal process, ensuring consent from survivors and utilizing evidence escrow for accountability.",
        "C": "Establish a court-mandated expedited removal route, with a standard procedure for all other cases."
      },
      "toolkit_cues": "How does speed vs. evidence retention affect survivors? What safe proof and appeal paths exist?",
      "p3_cues": "People (re-trauma risk), Planet (storage), Parity (legal access pathways).",
      "toolkit_flow": {
        "order": [],
        "prompts": [],
        "quick_actions": {
          "A": [
            "Automate removal within SLA",
            "Store cryptographic hashes only",
            "Provide survivor status updates"
          ],
          "B": [
            "Obtain survivor consent preferences",
            "Use evidence escrow chain-of-custody",
            "Time-box review to hard deadlines"
          ],
          "C": [
            "Define expedited legal intake",
            "Standardize judge order templates",
            "Monitor equity of access to pathway"
          ]
        },
        "metrics": [
          "Removal time",
          "Survivor satisfaction",
          "Investigation success rate"
        ],
        "owner_required": true,
        "review_default_days": 30
      },
      "toolkit_references": "Impact Explorer, Ethics Frame"
    }
  ]
}
